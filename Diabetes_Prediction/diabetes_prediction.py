# -*- coding: utf-8 -*-
"""Diabetes_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pm_daqzb6gYPc1iXw3R3g67utXmh9AnH
"""

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier

from google.colab import drive
drive.mount('/content/drive')

dataframe=pd.read_csv('/content/drive/MyDrive/Certificates/Internship/Offer Letters/Internship/TechnoHack/diabetes.zip')

dataframe

rows,columns= dataframe.shape
print(f'Number of rows: {rows} \nNumber of columns: {columns}')

dataframe.describe()

"""Data Cleaning"""

dataframe.isnull().sum()

"""Data Visualization"""

dataframe.hist(figsize=(10,10))
plt.show()

sns.pairplot(data=dataframe,hue='Outcome')
plt.show()

p=sns.catplot(x="Outcome",y="Age", data=dataframe, kind='box')
plt.title("Age and Outcome Correlation", size=20, y=1.0)

p=sns.catplot(x="Outcome",y="Glucose", data=dataframe, kind='box')
plt.title("Glucose and Outcome Correlation", size=20, y=1.0);

hig_corr = dataframe.corr()
hig_corr_features = hig_corr.index[abs(hig_corr["Outcome"]) >= 0.2]
hig_corr_features

"""Data Preprocessing"""

dataframe.var()

numeric_columns = ['Insulin', 'DiabetesPedigreeFunction',]

for column_name in numeric_columns:
    Q1 = np.percentile(dataframe[column_name], 25, interpolation='midpoint')
    Q3 = np.percentile(dataframe[column_name], 75, interpolation='midpoint')

    IQR = Q3 - Q1
    low_lim = Q1 - 1.5 * IQR
    up_lim = Q3 + 1.5 * IQR

    # Find outliers in the specified column
    outliers = dataframe[(dataframe[column_name] < low_lim) | (dataframe[column_name] > up_lim)][column_name]

    # Replace outliers with the respective lower or upper limit
    dataframe[column_name] = np.where(dataframe[column_name] < low_lim, low_lim, dataframe[column_name])
    dataframe[column_name] = np.where(dataframe[column_name] > up_lim, up_lim, dataframe[column_name])

X = dataframe.drop('Outcome', axis = 1)
y = dataframe['Outcome']

"""Splitting the Dataset"""

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20)

"""**Usng Machine Learning models** \
Logistic Regression Model
"""

from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(C=1, penalty='l2', solver='liblinear', max_iter=200)
log_reg.fit(X_train, y_train)

from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

def predict_and_plot(model, inputs, targets, name=''):
    preds = model.predict(inputs)
    accuracy = accuracy_score(targets, preds)
    print("Accuracy: {:.2f}%".format(accuracy * 100))

    cf = confusion_matrix(targets, preds, normalize='true')
    plt.figure()
    sns.heatmap(cf, annot=True)
    plt.xlabel('Prediction')
    plt.ylabel('Target')
    plt.title('{} Confusion Matrix'.format(name))

    return preds

# Predict and plot on the training data
train_preds = predict_and_plot(log_reg, X_train, y_train, 'Train')

# Predict and plot on the validation data
val_preds = predict_and_plot(log_reg, X_test, y_test, 'Validation')

"""Random Forest Model"""

from sklearn.ensemble import RandomForestClassifier
model_2 = RandomForestClassifier( random_state = 42)
model_2.fit(X_train,y_train)

model_2.score(X_train,y_train)

from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

def predict_and_plot(model, inputs, targets, name=''):
    preds = model.predict(inputs)
    accuracy = accuracy_score(targets, preds)
    print("Accuracy: {:.2f}%".format(accuracy * 100))

    cf = confusion_matrix(targets, preds, normalize='true')
    plt.figure()
    sns.heatmap(cf, annot=True)
    plt.xlabel('Prediction')
    plt.ylabel('Target')
    plt.title('{} Confusion Matrix'.format(name))

    return preds

# Predict and plot on the training data
train_preds = predict_and_plot(model_2, X_train, y_train, 'Train')

# Predict and plot on the validation data
val_preds = predict_and_plot(model_2, X_test, y_test, 'Validation')

"""Fitting the Random Forest Model"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

param_grid = {
    'n_estimators': [10, 20, 30],  # Adjust the number of trees in the forest
    'max_depth': [10, 20, 30],  # Adjust the maximum depth of each tree
    'min_samples_split': [2, 5, 10, 15, 20],  # Adjust the minimum samples required to split a node
    'min_samples_leaf': [1, 2, 4, 6, 8]  # Adjust the minimum samples required in a leaf node
}

model = RandomForestClassifier(random_state=42, n_jobs=-1)
grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, scoring='accuracy')
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

best_model.fit(X_train, y_train)

# Evaluate the model on the training and validation data
train_accuracy = best_model.score(X_train, y_train)
val_accuracy = best_model.score(X_test, y_test)

# Print the results
print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

"""SVM Model"""

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

svm_model = SVC(kernel='linear')

# Fit the SVM model to the training data
svm_model.fit(X_train, y_train)

def predict_and_plot(model, inputs, targets, name=''):
    preds = model.predict(inputs)
    accuracy = accuracy_score(targets, preds)
    print("Accuracy: {:.2f}%".format(accuracy * 100))

    cf = confusion_matrix(targets, preds, normalize='true')
    plt.figure()
    sns.heatmap(cf, annot=True)
    plt.xlabel('Prediction')
    plt.ylabel('Target')
    plt.title('{} Confusion Matrix'.format(name))

    return preds

# Predict and plot on the training data
train_preds = predict_and_plot(svm_model, X_train, y_train, 'Train')

# Predict and plot on the validation data
val_preds = predict_and_plot(svm_model, X_test, y_test, 'Validation')

"""XG_Boost Model"""

# Create an XGBoost classifier
xgboost_model = XGBClassifier(n_estimators=100, max_depth=3, random_state=42)

# Fit the XGBoost model to the training data
xgboost_model.fit(X_train, y_train)

# Make predictions on the training data
y_train_pred_xgboost = xgboost_model.predict(X_train)

# Make predictions on the validation data
y_val_pred_xgboost = xgboost_model.predict(X_val)

# Calculate the training accuracy
train_accuracy_xgboost = accuracy_score(y_train, y_train_pred_xgboost)

# Calculate the validation accuracy
val_accuracy_xgboost = accuracy_score(y_val, y_val_pred_xgboost)

# Print the training and validation accuracies
print("XGBoost Training Accuracy:", train_accuracy_xgboost)
print("XGBoost Validation Accuracy:", val_accuracy_xgboost)

"""***Conclusion***\
**Evaluate : Random Forest Model After Tunning**\
Training Accuracy: 0.9039087947882736\
Validation Accuracy: 0.8311688311688312
"""